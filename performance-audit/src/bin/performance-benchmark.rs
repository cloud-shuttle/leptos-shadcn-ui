//! Performance Benchmarking CLI Tool
//! 
//! This tool provides comprehensive performance benchmarking for leptos-shadcn-ui components
//! with automated regression testing and optimization recommendations.

use clap::{Parser, Subcommand};
use leptos_shadcn_performance_audit::{
    benchmarks::{BenchmarkRunner, BenchmarkConfig, BenchmarkSuiteResults},
    regression_testing::{RegressionTester, RegressionTestConfig, RegressionThresholds},
    automated_monitoring::{AutomatedMonitor, MonitoringConfig, AlertChannel},
    PerformanceAuditError,
};
use std::time::Duration;
use tokio::time::sleep;
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "performance-benchmark")]
#[command(about = "Performance benchmarking tool for leptos-shadcn-ui components")]
#[command(version = "1.0.0")]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Run performance benchmarks
    Benchmark {
        /// Number of benchmark iterations
        #[arg(short, long, default_value = "100")]
        iterations: u32,
        
        /// Target time in milliseconds
        #[arg(short, long, default_value = "16")]
        target_time: u64,
        
        /// Output format
        #[arg(short, long, default_value = "text")]
        format: String,
        
        /// Output file path
        #[arg(short, long)]
        output: Option<PathBuf>,
        
        /// Components to benchmark (comma-separated)
        #[arg(short, long)]
        components: Option<String>,
    },
    
    /// Run regression tests
    Regression {
        /// Baseline file path
        #[arg(short, long, default_value = "performance-baseline.json")]
        baseline: PathBuf,
        
        /// Results output path
        #[arg(short, long, default_value = "regression-results.json")]
        output: PathBuf,
        
        /// Auto-update baseline
        #[arg(long)]
        update_baseline: bool,
        
        /// Git commit hash
        #[arg(short, long)]
        commit: Option<String>,
    },
    
    /// Start automated monitoring
    Monitor {
        /// Monitoring interval in seconds
        #[arg(short, long, default_value = "30")]
        interval: u64,
        
        /// Enable alerts
        #[arg(long)]
        enable_alerts: bool,
        
        /// Alert webhook URL
        #[arg(long)]
        webhook_url: Option<String>,
        
        /// Alert email recipients (comma-separated)
        #[arg(long)]
        email_recipients: Option<String>,
        
        /// Monitoring duration in seconds (0 = infinite)
        #[arg(short, long, default_value = "0")]
        duration: u64,
    },
    
    /// Generate performance report
    Report {
        /// Input results file
        #[arg(short, long, default_value = "benchmark-results.json")]
        input: PathBuf,
        
        /// Output report file
        #[arg(short, long, default_value = "performance-report.html")]
        output: PathBuf,
        
        /// Report format
        #[arg(short, long, default_value = "html")]
        format: String,
    },
    
    /// Setup performance baseline
    Setup {
        /// Baseline file path
        #[arg(short, long, default_value = "performance-baseline.json")]
        output: PathBuf,
        
        /// Git commit hash
        #[arg(short, long)]
        commit: Option<String>,
    },
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let cli = Cli::parse();
    
    match cli.command {
        Commands::Benchmark { iterations, target_time, format, output, components } => {
            run_benchmarks(iterations, target_time, format, output, components).await?;
        }
        Commands::Regression { baseline, output, update_baseline, commit } => {
            run_regression_tests(baseline, output, update_baseline, commit).await?;
        }
        Commands::Monitor { interval, enable_alerts, webhook_url, email_recipients, duration } => {
            run_monitoring(interval, enable_alerts, webhook_url, email_recipients, duration).await?;
        }
        Commands::Report { input, output, format } => {
            generate_report(input, output, format).await?;
        }
        Commands::Setup { output, commit } => {
            setup_baseline(output, commit).await?;
        }
    }
    
    Ok(())
}

/// Run performance benchmarks
async fn run_benchmarks(
    iterations: u32,
    target_time: u64,
    format: String,
    output: Option<PathBuf>,
    components: Option<String>,
) -> Result<(), Box<dyn std::error::Error>> {
    println!("üèÉ Running performance benchmarks...");
    println!("   Iterations: {}", iterations);
    println!("   Target time: {}ms", target_time);
    
    let config = BenchmarkConfig {
        warmup_iterations: 10,
        benchmark_iterations: iterations,
        target_time: Duration::from_millis(target_time),
        enable_memory_profiling: true,
        enable_statistical_analysis: true,
    };
    
    let mut runner = BenchmarkRunner::new(config);
    
    // Register benchmarks for specified components or all components
    let components_to_test = if let Some(components_str) = components {
        components_str.split(',').map(|s| s.trim().to_string()).collect()
    } else {
        vec!["button".to_string(), "input".to_string(), "select".to_string(), "card".to_string(), "badge".to_string()]
    };
    
    for component in components_to_test {
        println!("   Registering benchmarks for {}...", component);
        // This would register actual component benchmarks
        // For now, we'll use mock benchmarks
        let benchmark = Box::new(MockBenchmark {
            name: format!("{}-render", component),
            component_name: component.clone(),
            execution_time: Duration::from_millis(target_time / 2), // Simulate good performance
            memory_usage: 1024,
        });
        runner.register_benchmark(benchmark);
    }
    
    let results = runner.run_all_benchmarks().await;
    
    // Output results
    match format.as_str() {
        "json" => {
            let json_output = serde_json::to_string_pretty(&results)?;
            if let Some(output_path) = output {
                std::fs::write(&output_path, json_output)?;
                println!("‚úÖ Results saved to {:?}", output_path);
            } else {
                println!("{}", json_output);
            }
        }
        "html" => {
            let html_output = generate_html_report(&results);
            if let Some(output_path) = output {
                std::fs::write(&output_path, html_output)?;
                println!("‚úÖ HTML report saved to {:?}", output_path);
            } else {
                println!("{}", html_output);
            }
        }
        _ => {
            print_text_results(&results);
        }
    }
    
    // Check if benchmarks passed
    if results.meets_targets() {
        println!("‚úÖ All benchmarks passed!");
        std::process::exit(0);
    } else {
        println!("‚ùå Some benchmarks failed!");
        std::process::exit(1);
    }
}

/// Run regression tests
async fn run_regression_tests(
    baseline: PathBuf,
    output: PathBuf,
    update_baseline: bool,
    commit: Option<String>,
) -> Result<(), Box<dyn std::error::Error>> {
    println!("üìä Running performance regression tests...");
    
    let config = RegressionTestConfig {
        baseline_path: baseline.to_string_lossy().to_string(),
        results_path: output.to_string_lossy().to_string(),
        thresholds: RegressionThresholds::default(),
        auto_update_baseline: update_baseline,
        generate_recommendations: true,
    };
    
    let mut tester = RegressionTester::new(config);
    
    // Load baseline
    if let Err(e) = tester.load_baseline().await {
        println!("‚ö†Ô∏è Failed to load baseline: {}", e);
        println!("   Run 'performance-benchmark setup' to create a baseline");
        return Ok(());
    }
    
    // Run current benchmarks (simplified)
    let current_results = BenchmarkSuiteResults::default();
    
    // Run regression analysis
    let regression_results = tester.run_regression_tests(&current_results).await?;
    
    // Save results
    tester.save_results(&regression_results).await?;
    
    // Generate report
    let report = tester.generate_report(&regression_results);
    println!("{}", report);
    
    // Check for regressions
    let critical_regressions = regression_results.iter()
        .filter(|r| r.severity == leptos_shadcn_performance_audit::regression_testing::RegressionSeverity::Critical)
        .count();
    
    let major_regressions = regression_results.iter()
        .filter(|r| r.severity == leptos_shadcn_performance_audit::regression_testing::RegressionSeverity::Major)
        .count();
    
    if critical_regressions > 0 {
        println!("üö® CRITICAL: {} critical regressions detected!", critical_regressions);
        std::process::exit(1);
    } else if major_regressions > 0 {
        println!("‚ö†Ô∏è WARNING: {} major regressions detected!", major_regressions);
        std::process::exit(1);
    } else {
        println!("‚úÖ No significant regressions detected!");
    }
    
    Ok(())
}

/// Run automated monitoring
async fn run_monitoring(
    interval: u64,
    enable_alerts: bool,
    webhook_url: Option<String>,
    email_recipients: Option<String>,
    duration: u64,
) -> Result<(), Box<dyn std::error::Error>> {
    println!("üìà Starting automated performance monitoring...");
    println!("   Interval: {}s", interval);
    println!("   Alerts enabled: {}", enable_alerts);
    
    let mut alert_channels = vec![AlertChannel::Console];
    
    if let Some(url) = webhook_url {
        alert_channels.push(AlertChannel::Webhook { url });
    }
    
    if let Some(recipients) = email_recipients {
        let recipient_list = recipients.split(',').map(|s| s.trim().to_string()).collect();
        alert_channels.push(AlertChannel::Email { recipients: recipient_list });
    }
    
    let config = MonitoringConfig {
        monitoring_interval: Duration::from_secs(interval),
        alert_thresholds: leptos_shadcn_performance_audit::automated_monitoring::AlertThresholds {
            performance_degradation_threshold: 10.0,
            memory_usage_threshold: 20.0,
            bundle_size_threshold: 15.0,
            error_rate_threshold: 5.0,
        },
        retention_period: Duration::from_secs(24 * 60 * 60), // 24 hours
        enable_alerts,
        alert_channels,
    };
    
    let monitor = AutomatedMonitor::new(config);
    
    // Start monitoring
    monitor.start_monitoring().await?;
    
    if duration > 0 {
        println!("   Monitoring for {} seconds...", duration);
        sleep(Duration::from_secs(duration)).await;
        monitor.stop_monitoring().await?;
        println!("‚úÖ Monitoring completed!");
    } else {
        println!("   Monitoring indefinitely (Ctrl+C to stop)...");
        // Keep running until interrupted
        tokio::signal::ctrl_c().await?;
        monitor.stop_monitoring().await?;
        println!("‚úÖ Monitoring stopped!");
    }
    
    Ok(())
}

/// Generate performance report
async fn generate_report(
    input: PathBuf,
    output: PathBuf,
    format: String,
) -> Result<(), Box<dyn std::error::Error>> {
    println!("üìÑ Generating performance report...");
    println!("   Input: {:?}", input);
    println!("   Output: {:?}", format);
    
    if !input.exists() {
        return Err(format!("Input file not found: {:?}", input).into());
    }
    
    let content = std::fs::read_to_string(input)?;
    let results: BenchmarkSuiteResults = serde_json::from_str(&content)?;
    
    match format.as_str() {
        "html" => {
            let html_report = generate_html_report(&results);
            std::fs::write(&output, html_report)?;
        }
        "markdown" => {
            let markdown_report = generate_markdown_report(&results);
            std::fs::write(&output, markdown_report)?;
        }
        _ => {
            return Err(format!("Unsupported format: {}", format).into());
        }
    }
    
    println!("‚úÖ Report generated: {:?}", output);
    Ok(())
}

/// Setup performance baseline
async fn setup_baseline(
    output: PathBuf,
    commit: Option<String>,
) -> Result<(), Box<dyn std::error::Error>> {
    println!("üîß Setting up performance baseline...");
    
    let commit_hash = commit.unwrap_or_else(|| "unknown".to_string());
    
    // Run benchmarks to establish baseline
    let config = BenchmarkConfig {
        warmup_iterations: 10,
        benchmark_iterations: 100,
        target_time: Duration::from_millis(16),
        enable_memory_profiling: true,
        enable_statistical_analysis: true,
    };
    
    let mut runner = BenchmarkRunner::new(config);
    
    // Register all component benchmarks
    let components = vec!["button", "input", "select", "card", "badge"];
    for component in components {
        let benchmark = Box::new(MockBenchmark {
            name: format!("{}-render", component),
            component_name: component.to_string(),
            execution_time: Duration::from_millis(8), // Simulate good performance
            memory_usage: 1024,
        });
        runner.register_benchmark(benchmark);
    }
    
    let results = runner.run_all_benchmarks().await;
    
    // Create regression tester and update baseline
    let regression_config = RegressionTestConfig {
        baseline_path: output.to_string_lossy().to_string(),
        results_path: "temp-results.json".to_string(),
        thresholds: RegressionThresholds::default(),
        auto_update_baseline: true,
        generate_recommendations: true,
    };
    
    let mut tester = RegressionTester::new(regression_config);
    tester.update_baseline(&results, &commit_hash).await?;
    
    println!("‚úÖ Performance baseline established: {:?}", output);
    println!("   Commit: {}", commit_hash);
    println!("   Components: {}", results.benchmark_results.len());
    
    Ok(())
}

/// Print text results
fn print_text_results(results: &BenchmarkSuiteResults) {
    println!("\nüìä Benchmark Results:");
    println!("===================");
    
    for (name, result) in &results.benchmark_results {
        println!("\n{} ({})", result.component_name, name);
        println!("  Average time: {:.2}ms", result.average_time.as_secs_f64() * 1000.0);
        println!("  Min time: {:.2}ms", result.min_time.as_secs_f64() * 1000.0);
        println!("  Max time: {:.2}ms", result.max_time.as_secs_f64() * 1000.0);
        println!("  Memory usage: {} bytes", result.memory_usage_bytes);
        println!("  Performance score: {:.1}/100", result.performance_score);
        println!("  Meets target: {}", if result.meets_target { "‚úÖ" } else { "‚ùå" });
    }
    
    println!("\nOverall Score: {:.1}/100", results.overall_score);
    println!("Failing components: {}", results.failing_components.len());
}

/// Generate HTML report
fn generate_html_report(results: &BenchmarkSuiteResults) -> String {
    format!(r#"<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Performance Benchmark Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f5f5f5; padding: 20px; border-radius: 5px; }}
        .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }}
        .metric {{ background: white; padding: 15px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .metric h3 {{ margin: 0 0 10px 0; color: #333; }}
        .metric .value {{ font-size: 2em; font-weight: bold; }}
        .success {{ color: #28a745; }}
        .failure {{ color: #dc3545; }}
        .results {{ margin: 20px 0; }}
        .result {{ background: white; margin: 10px 0; padding: 15px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Performance Benchmark Report</h1>
        <p>Generated: {}</p>
    </div>

    <div class="summary">
        <div class="metric">
            <h3>Overall Score</h3>
            <div class="value {}">{:.1}/100</div>
        </div>
        <div class="metric">
            <h3>Total Tests</h3>
            <div class="value">{}</div>
        </div>
        <div class="metric">
            <h3>Passing Tests</h3>
            <div class="value success">{}</div>
        </div>
        <div class="metric">
            <h3>Failing Tests</h3>
            <div class="value failure">{}</div>
        </div>
    </div>

    <div class="results">
        <h2>Detailed Results</h2>
        {}
    </div>
</body>
</html>"#,
        chrono::Utc::now().format("%Y-%m-%d %H:%M:%S UTC"),
        if results.overall_score >= 80.0 { "success" } else { "failure" },
        results.overall_score,
        results.benchmark_results.len(),
        results.benchmark_results.values().filter(|r| r.meets_target).count(),
        results.benchmark_results.values().filter(|r| !r.meets_target).count(),
        results.benchmark_results.iter().map(|(name, result)| {
            format!(r#"
        <div class="result">
            <h3>{} ({})</h3>
            <p><strong>Average time:</strong> {:.2}ms</p>
            <p><strong>Memory usage:</strong> {} bytes</p>
            <p><strong>Performance score:</strong> {:.1}/100</p>
            <p><strong>Status:</strong> {}</p>
        </div>"#,
                result.component_name, name,
                result.average_time.as_secs_f64() * 1000.0,
                result.memory_usage_bytes,
                result.performance_score,
                if result.meets_target { "‚úÖ Pass" } else { "‚ùå Fail" }
            )
        }).collect::<Vec<_>>().join("")
    )
}

/// Generate markdown report
fn generate_markdown_report(results: &BenchmarkSuiteResults) -> String {
    format!(r#"# Performance Benchmark Report

**Generated**: {}

## Summary

- **Overall Score**: {:.1}/100
- **Total Tests**: {}
- **Passing Tests**: {}
- **Failing Tests**: {}

## Detailed Results

{}

---
*Report generated by performance-benchmark tool*
"#,
        chrono::Utc::now().format("%Y-%m-%d %H:%M:%S UTC"),
        results.overall_score,
        results.benchmark_results.len(),
        results.benchmark_results.values().filter(|r| r.meets_target).count(),
        results.benchmark_results.values().filter(|r| !r.meets_target).count(),
        results.benchmark_results.iter().map(|(name, result)| {
            format!(r#"
### {} ({})

- **Average time**: {:.2}ms
- **Memory usage**: {} bytes
- **Performance score**: {:.1}/100
- **Status**: {}

"#,
                result.component_name, name,
                result.average_time.as_secs_f64() * 1000.0,
                result.memory_usage_bytes,
                result.performance_score,
                if result.meets_target { "‚úÖ Pass" } else { "‚ùå Fail" }
            )
        }).collect::<Vec<_>>().join("")
    )
}

/// Mock benchmark for testing
struct MockBenchmark {
    name: String,
    component_name: String,
    execution_time: std::time::Duration,
    memory_usage: u64,
}

impl leptos_shadcn_performance_audit::benchmarks::Benchmark for MockBenchmark {
    fn name(&self) -> &str {
        &self.name
    }
    
    fn component_name(&self) -> &str {
        &self.component_name
    }
    
    fn run(&self, iterations: u32) -> leptos_shadcn_performance_audit::benchmarks::BenchmarkResult {
        let mut result = leptos_shadcn_performance_audit::benchmarks::BenchmarkResult::new(
            self.name.clone(),
            self.component_name.clone(),
        );
        
        result.average_time = self.execution_time;
        result.min_time = self.execution_time;
        result.max_time = self.execution_time;
        result.memory_usage_bytes = self.memory_usage;
        result.iterations = iterations;
        result.calculate_performance_score(std::time::Duration::from_millis(16));
        
        result
    }
}
